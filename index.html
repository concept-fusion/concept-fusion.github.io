<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ConceptFusion: Open-set Multimodal 3D Mapping">
  <meta name="keywords" content="ConceptFusion, 3D Mapping, SLAM, Open-set, Multimodal, Foundation models, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ConceptFusion: Open-set Multimodal 3D Mapping</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JX0F75QDW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://concept-fusion.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://gradslam.github.io">
            GradSLAM
          </a>
          <a class="navbar-item" href="https://mahis.life/clip-fields/">
            CLIP-Fields
          </a>
          <a class="navbar-item" href="https://pengsongyou.github.io/openscene">
            OpenScene
          </a>
          <a class="navbar-item" href="https://say-can.github.io/">
            Say-Can
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">ConceptFusion: Open-set Multimodal 3D Mapping</h1>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://krrish94.github.io">Krishna Murthy Jatavallabhula</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.alihkw.com/">Alihusein Kuwajerwala</a><sup>2</sup><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://georgegu1997.github.io/">Qiao Gu</a><sup>3</sup><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=jFH3ShsAAAAJ&hl=en">Mohd Omama</a><sup>4</sup><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://taochenshh.github.io/">Tao Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://epiception.github.io/">Ganesh Iyer</a><sup>6</sup><sup><span>&#8224;</span></sup>,
            </span>
            <span class="author-block">
              <a href="https://saryazdi.github.io/">Soroush Saryazdi</a><sup>7</sup><sup><span>&#8224;</span></sup>,
            </span>
            <span class="author-block">
              <a href="https://nik-v9.github.io/">Nikhil Keetha</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://ayushtewari.com/">Ayush Tewari</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://celsodemelo.net/">Celso Miguel de Melo</a><sup>8</sup>,
            </span>
            <span class="author-block">
              <a href="https://robotics.iiit.ac.in/">Madhava Krishna</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="http://liampaull.ca">Liam Paull</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.cs.toronto.edu/\~florian/">Florian Shkurti</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>MIT,</span>
            <span class="author-block"><sup>2</sup>Université de Montréal,</span>
            <span class="author-block"><sup>3</sup>University of Toronto,</span>
            <span class="author-block"><sup>4</sup>IIIT Hyderabad,</span>
            <span class="author-block"><sup>5</sup>CMU,</span>
            <span class="author-block"><sup>6</sup>Amazon,</span>
            <span class="author-block"><sup>7</sup>Matician,</span>
            <span class="author-block"><sup>8</sup>DEVCOM Army Research Laboratory</span>
          </div>

          <div class="is-size-7 publication-authors">
            <span class="author-block"><sup>*</sup>Co-second authors</span>
            <span class="author-block"><sup>&#8224;</sup>Work done prior to current affiliation</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="assets/pdf/2023-ConceptFusion.pdf"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal " disabled="true">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=rkXgws8fiDs"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/concept-fusion"
                   class="external-link button is-normal " disabled="true">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal" disabled="true">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
        <img source src="./static/images/splash.gif" />
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        <span class="coolname">ConceptFusion</span> builds open-set 3D maps that can be queried via text, click, image, or audio.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" id="clustering" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clustering.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="roundabout-nav" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/roundabout_nav.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-baymax" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm_baymax.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="text-query-somewhere-to-sit" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/text_query_somewhere_to_sit.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="image-query-cabinet" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/image_query_cabinet.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="audio-query-doorknock" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/audio_query_doorknock.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="click-query-cabinet" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/click_query_cabinet.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="clustering-outdoors" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clustering_outdoors.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="football-field-nav" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/football_field_nav.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-caterpillar" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm_caterpillar.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-purple" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm_purple.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-spindrift" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm_spindrift.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/rkXgws8fiDs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason  bout a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts.
          </p>
          <p>
            We address both these issues with ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Construct pixel-aligned features</h3>
        <div class="content has-text-justified">
          <p>
            ConceptFusion constructs pixel-aligned features from off-the-shelf foundation models that can only produce a global (image-level) embedding vector. This is achieved by: processing input images to generate generic (class-agnostic) object masks and extracting a local features for each, computing a global feature for the input image as a whole, and fusing the region-specific features with global features using our proposed zero-shot pixel alignment technique.
          </p>
          <img src="./static/images/pipeline.png" />
        </div>
        <br/>
      </div>
    </div>


    <div class="columns is-centered">

      <!-- Zero-shot pixel alignment -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Zero-shot pixel alignment</h2>
          <img src="./static/images/pixel-aligned-feature-computation.png" />
          <p>
            For each image, the global (f<sub>G</sub>) and local (f<sub>L</sub>) features are fused to obtain our pixel-aligned features (f<sub>P</sub>). <i>Top-left</i>: We first compute cosine similarities between each local feature (f<sub>L</sub>) with the global feature (f<sub>G</sub>). <i>Top-right</i>: We compute an inter-feature similarity matrix, and compute the average similarity of each local feature to every other local feature, denoted φ̄<sub>i</sub> . <i>Bottom-left</i>: We combine these similarities to produce weights for fusing f<sub>G</sub> and f<sub>L</sub> to obtain pixel-aligned features f<sub>P</sub>.
          </p>
        </div>
      </div>
      <!--/ Zero-shot pixel alignment -->

      <!-- Long-tailed concepts -->
      <div class="column">
        <h2 class="title is-3">Retaining fine-grained concepts</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/pixel-aligned-qualitative.png" />
            <p>
              Our approach to computing pixel-aligned features is adept at capturing long-tailed and fine-grained concepts. The plots to the right show the similarity scores between the embeddings of the cropped image regions corresponding to diet coke, lysol, and yogurt and their text embeddings, predicted by the base CLIP model used by LSeg and OpenSeg respectively. This implies that the base CLIP models know these concepts, yet, as can be seen on the tiled plots (center), LSeg and OpenSeg are not able to retrieve these concepts; they forget the concepts when finetuned. On the other hand, our zero-shot pixel-alignment approach does not suffer this drawback, and clearly delineates the corresponding pixels.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Long-tailed concepts -->


    <div class="columns is-centered">

      <!-- UnCoCo dataset -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">UnCoCo dataset</h2>
          <img src="./static/images/uncoco.png" />
          <p>
            To evaluate long-tailed reasoning and multimodal reasoning abilities, since there is no existing dataset, we capture a set of 20 RGB-D sequences comprising 78 commonly found objects and annotate them with text, audio, click, and image queries. For each query, we also provide the corresponding ground truth 2D and 3D retrieval results. This image showcases sample tabletop scenes from UnCoCo (left) and the resulting 3D reconstructions and labels (right).
          </p>
        </div>
      </div>
      <!--/ UnCoCo dataset -->

      <!-- 3D spatial reasoning -->
      <div class="column">
        <h2 class="title is-3">3D spatial reasoning</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/spatial-query.png" />
            <p>
              <i>What is the distance between the refrigerator from the television?</i>
            </p>
            <p>
              A key benefit of lifting foundation features to 3D is the ability to reason about spatial attributes. We implement a set of generic spatial relationship comparators that can be leveraged for querying arbitrary objects. We employ a large language model to parse the queries to generate function calls that can directly be executed. E.g., the query above parses to <tt>howFar(refrigirator, television)</tt>.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ 3D spatial reasoning -->

    <!-- Long-form text queries -->    
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Interpolating. -->
        <h2 class="title is-4">Long-form text queries</h2>
        <div class="content has-text-justified">
          <p>
            ConceptFusion is able to handle long-form text queries and accurately localize objects referenced by the query. In the first two scenarios, OpenSeg is distracted by the presence of several confounding attributes. The third scenario shows a single world query (television) that is part of the COCO Captions dataset used to train OpenSeg, providing it an unfair advantage. ConceptFusion, nonetheless, accurately assigns the highest response to the map points representing the television. In each query, the referenced object is boldfaced.
          </p>
          <img src="./static/images/scannet-text-query.png" />
        </div>
        <br/>
      </div>
    </div>
    <!--/ Long-form text queries -->

    <!-- Click-query video -->    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Click-queries</h2>
        <div class="content has-text-justified">
          <p>
            Click-queries over a sequence from the ICL dataset. For each clicked point, we compute the cosine similarity of the embedding at that point with that of every other map point and visualize them using a 'jet' colormap. Points in red indicate greatest similarity, while points in blue indicate least similarity. Notice the consistency in semantic concepts. For instance, when we click on a point on the corner lamp (at about 0:45), we also notice that the other corner lamp, as well as lights on top of the ceiling get high similarities assigned.
          </p>
          <video id="click-query-icl" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/ICL Click query cropped.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Click-query video -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments on real robotic systems</h2>
    <div class="columns is-centered">

      <!-- Tabletop rearrangement -->
      <div class="column">
        <div class="content">
          <h3 class="title is-3">Tabletop rearrangement</h3>
          <img src="./static/images/rearrangement.png" />
          <p>
            The robot is provided with rearrangment goals involving novel objects. (Top row) push goldfish to the right of the yellow line, where goldfish refers to the brandname of the pack of Cheddar snack. (Bottom row) push baymax to the right of the yellow line, where baymax refers to the plush toy depicting the famous Disney character.
          </p>
        </div>
      </div>
      <!--/ Tabletop rearrangement -->

      <!-- Self-driving -->
      <div class="column">
        <h3 class="title is-3">Autonomous driving</h3>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/driving.png" />
            <p>
              (Left to right; top to bottom) Autonomous drive-by-wire platform deployed; pointcloud map of the environment with the response to the openset text-query ”football field” (shown in red); path found to the football field (shown in red); car successfully navigates to the destination autonomously. See our anonymized webpage for more results.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Self-driving -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Integrating ConceptFusion with Large Language Models</h2>
    <div class="columns is-centered">

      <!-- GPT video 1 -->
      <div class="column">
        <div class="content">
          <video id="gpt-video-1" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt-video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ GPT video 1 -->

      <!-- GPT video 2 -->
      <div class="column">
        <div class="content">
          <video id="gpt-video-2" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt-video2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ GPT video 2 -->
    </div>
  </div>
</section>


<section class="section" id="concurrent work">
  <div class="container is-max-desktop content">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Concurrent work</h2>

        <div class="content has-text-justified">
          <p>
            Given the pace of AI research these days, it is extremely challenging to keep up with all of the work around foundation models and open-set perception. We list below a few key approaches that we have come across after beginning work on ConceptFusion. If we may have inadvertently missed out on key concurrent work, please reach out to us over email (or better, open a pull request on <a href="https://github.com/concept-fusion/concept-fusion.github.io">our GitHub page</a>).
          </p>
          <p>
            <a href="https://mahis.life/clip-fields/">CLIP-Fields</a> encodes features from language and vision-language models into a compact, scene-specific neural network trained to predict feature embeddings from 3D point coordinates; to enable open-set visual understanding tasks.
          </p>
          <p>
            <a href="https://pengsongyou.github.io/openscene">OpenScene</a> demonstrates that features from pixel-aligned 2D vision-language models can be distilled to 3D, generalize to new scenes, and perform better than their 2D counterparts.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2211.16312">Deng et al.</a> demonstrate interesting ways of learning hierarchical scene abstractions by distilling features from 2D vision-language foundation models, and smart ways of interpreting captions from 2D captioning approaches.
          </p>
          <p>
            <a href="https://makezur.github.io/FeatureRealisticFusion/">Feature-realistic neural fusion</a> demonstrates the integration of DINO features into a real-time neural mapping and positioning system.
          </p>
          <p>
            <a href="https://say-can.github.io/">Say-Can</a> demonstrates the applicability of large language models as task-planners, and leverage a set of low-level skills to execute these plans in the real world. Also related to this line of work are <a href="https://vlmaps.github.io/">VL-Maps</a>, <a href="https://nlmap-saycan.github.io/">NLMap-SayCan</a>, and <a href="https://cow.cs.columbia.edu/">CoWs</a>, which demonstrate the benefits of having a map queryable via language.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<script type="text/javascript">
  $(function() {
  var screenWidth = $(window).width();
  if (screenWidth >= 800) {
    $('#gpt-video-1').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#gpt-video-2').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#click-query-icl').attr('autoplay', 'autoplay');
  }
});
</script>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{conceptfusion,
  author    = {Jatavallabhula, {Krishna Murthy} and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, {Joshua B.} and {de Melo}, {Celso Miguel} and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio},
  title     = {ConceptFusion: Open-set Multimodal 3D Mapping},
  journal   = {arXiv},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./assets/pdf/2023-ConceptFusion.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/concept-fusion" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website adapted from the Nerfies templates, which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/conceptfusion/conceptfusion.github.io">source code</a> of this website,
            we just ask that you link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a> in the footer.
            Please remember to remove the analytics code included in the header of the website which you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
